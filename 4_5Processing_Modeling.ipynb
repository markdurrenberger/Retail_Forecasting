{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Preprocessing & Feature Engineering\n",
    "\n",
    "Load the final data from steps 2 & 3, create our target variable (and any other relevant features), then scale and prepare data for modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load final data\n",
    "data = pd.read_csv('data/processed/data.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['Unnamed: 0','customer.1'], inplace=True, axis=1)\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Set Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index will be multi-index: customer and invoice date\n",
    "data['invoice_date'] = pd.to_datetime(data['invoice_date'])\n",
    "data.set_index(['customer','invoice_date'], inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Create Target Feature\n",
    "\n",
    "The target feature will be a y/n - was this customer's purchase of a higher priced item than any previous purchases. There will be several steps towards creating this:\n",
    "\n",
    "* Creating a ranking of items by average price - different customers have different purchase prices, so using the average price paid should help even that out.\n",
    "\n",
    "* Using that information to add a column with \"highest lifetime item\" - this will list the highest priced item that customer has purchased in their history to that point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1 Create ranking of items by average price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What's the spread of number of items sold?\n",
    "\n",
    "# Create a series with the number of items sold by item\n",
    "item_count = data.groupby('description')['item_price'].count().to_frame()\n",
    "item_count.sort_values('item_price', ascending=False, inplace = True)\n",
    "item_count.columns = ['num_times_sold']\n",
    "item_count.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Find a threshhold below-which it makes sense to drop\n",
    "sns.countplot(item_count['num_times_sold'])\n",
    "plt.title('Spread of number of items sold by item')\n",
    "plt.xlabel('Total # of item sold')\n",
    "plt.ylabel('Count of the number of items sold per quantity')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/item_countplot.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above plot, there are lots of items that only sold once. Those we don't necessarily want, but this doesn't give us any clear \"cut off\" point. Let's cut off items only sold once at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop items only sold once\n",
    "item_count = item_count[item_count['num_times_sold']>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop 'Rental' and 'Misc Hardware'\n",
    "item_count.drop(['RENTAL','MISCELLANEOUS HARDWARE'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use that list of items to drop from original data list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a list of the items to keep (the index from item_count)\n",
    "keep_list = list(item_count.index)\n",
    "\n",
    "# Use that to filter the data df and keep only those items (description isin list)\n",
    "data = data[data['description'].isin(keep_list)]\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use this updated list to calculate and rank items by average price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = data.groupby('description')['item_price'].mean().to_frame()\n",
    "items.columns = ['avg_sale_price']\n",
    "items.sort_values('avg_sale_price', ascending=False, inplace=True)\n",
    "\n",
    "# Create top lists for examination\n",
    "top_25 = items.head(25)\n",
    "top_50 = items.head(50)\n",
    "top_100 = items.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the top 25 list\n",
    "sns.barplot(x=top_25.index, y=top_25.avg_sale_price)\n",
    "plt.title('Avg Sales Price for Top 25 Priced Items')\n",
    "plt.ylabel('Avg Sale Price')\n",
    "plt.xticks(rotation=85)\n",
    "plt.xlabel('Item Name')\n",
    "\n",
    "plt.savefig('figures/top25_items.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop \"Payment\" items\n",
    "These payment items are almost all the highest priced items and throw things off significantly. Drop anything with \"payment\" in the name\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Drop any item with \"payment\" in the name\n",
    "items.reset_index(inplace=True)\n",
    "\n",
    "items = items[~items['description'].str.contains('PAYMENT')]\n",
    "items.set_index('description', inplace=True)\n",
    "items.head()\n",
    "\n",
    "# Create top lists for examination\n",
    "top_25 = items.head(25)\n",
    "top_50 = items.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the top 25 list\n",
    "sns.barplot(x=top_25.index, y=top_25.avg_sale_price)\n",
    "plt.title('UPDATED - Avg Sales Price for Top 25 Priced Items')\n",
    "plt.ylabel('Avg Sale Price')\n",
    "plt.xticks(rotation=85)\n",
    "plt.xlabel('Item Name')\n",
    "\n",
    "plt.savefig('figures/top25_items_no_payment_plans.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column that is the item rank\n",
    "items.reset_index(inplace=True)\n",
    "items['item_price_rank'] = [i+1 for i in items.index]\n",
    "items.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop really low priced items too\n",
    "\n",
    "How to define \"really low?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items.tail(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last 200 items are still largely accessories - all priced under $40. That seems like a good enough threshold for now. We can always adjust this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all items priced under $40\n",
    "items = items[items['avg_sale_price']>40]\n",
    "items.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2 Keep only the items left in original dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_list = list(items.description.unique())\n",
    "data = data[data['description'].isin(keep_list)]\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.3 Join the item_price_rank with data df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join item sales rank with data\n",
    "data = pd.merge(data.reset_index(), items[['description', 'item_price_rank']], \n",
    "                on='description', how='inner')\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.4 Drop all but highest priced item for each sale date\n",
    "\n",
    "Each row of my final data frame will represent the highest ranked item purchased on that sale date by that customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all but the highest priced item for each sale date/customer combo\n",
    "\n",
    "# Make sure data are sorted by customer, then invoice_date, then item_price descending\n",
    "data.sort_values(by=['customer','invoice_date','item_price'], ascending=False, inplace=True)\n",
    "\n",
    "# Drop duplicate (customer/invoice_date) combos, keeping the first (i.e. highest priced)\n",
    "data.drop_duplicates(subset=['customer','invoice_date'], keep='first', inplace=True)\n",
    "\n",
    "# Reset index\n",
    "data.set_index(['customer','invoice_date'], inplace=True)\n",
    "\n",
    "# Confirm only one item per date per customer\n",
    "data.sort_index(level='customer')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.5 Drop unnessecary columns\n",
    "\n",
    "Since we only needed the item_price to create this ranking, we can drop that column now. (It would be highly correlated with item_price_rank anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop('item_price',axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.6 Create the Target Feature Column\n",
    "\n",
    "Now we're ready to create our target feature, which addressing the question: is this the highest priced item purchased by this customer to date?\n",
    "\n",
    "(Another way of saying, is this the highest ranked item by sales price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data[['item_price_rank']]\n",
    "df.reset_index(inplace=True)\n",
    "df.sort_values(['customer','invoice_date'], ascending=[False, True], inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through the invoice dates for each customer & assign 1 to any purchase ranked higher than\n",
    "# prior purchases\n",
    "\n",
    "# this will be the list that becomes target feature\n",
    "target_list = []\n",
    "\n",
    "# Loop through each customer\n",
    "for customer in df.customer.unique():\n",
    "    cust_df = df[df['customer']==customer]\n",
    "    \n",
    "    # counter\n",
    "    i = 0\n",
    "    \n",
    "    for date in cust_df.invoice_date.values:\n",
    "        # If first item purchased, assign 0\n",
    "        if i == 0:\n",
    "            target_list.append(0)\n",
    "            i += 1\n",
    "            continue\n",
    "        \n",
    "        # For susequent purchases, was this item higher ranked than any previous\n",
    "        item_max = cust_df.item_price_rank.iloc[:i-1].max()\n",
    "        \n",
    "        if cust_df.item_price_rank.iloc[i] > item_max:\n",
    "            target_list.append(1)\n",
    "            i += 1\n",
    "            continue\n",
    "        else:\n",
    "            target_list.append(0)\n",
    "            i += 1\n",
    "            continue  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# double check\n",
    "target_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create target feature\n",
    "data['target'] = target_list\n",
    "display(data.info())\n",
    "display(data.target.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.7 Upsample positive class\n",
    "\n",
    "The data is very imbalanced, so I will upsample the positive class to help balance that out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "# separate classes\n",
    "data_minority = data[data['target']==1]\n",
    "data_majority = data[data['target']==0]\n",
    "\n",
    "# upsample minority\n",
    "data_minority_upsampled = resample(data_minority, \n",
    "                                 replace=True,      # sample with replacement\n",
    "                                 n_samples=1773,    # to match majority class\n",
    "                                 random_state=2)    # reproducible results)\n",
    "\n",
    "# combine upsampled results with majority class\n",
    "data_upsampled = pd.concat([data_majority, data_minority_upsampled])\n",
    "\n",
    "# check\n",
    "data_upsampled.target.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to file\n",
    "data_upsampled.to_csv('data/processed/data_upsampled.csv')\n",
    "data.to_csv('data/processed/data_final_w_target.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Fix Column Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reloading data if I screw up the df below\n",
    "#data = pd.read_csv('data/processed/data_upsampled.csv', \n",
    "#                   index_col=['customer','invoice_date'])\n",
    "\n",
    "data = data_upsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In all that engineering we lost our data types\n",
    "data['industry'] = data['industry'].astype('category')\n",
    "data['contact_owner'] = data['contact_owner'].astype('category')\n",
    "data['county'] = data['county'].astype('category')\n",
    "\n",
    "data['first_sale'] = pd.to_datetime(data['first_sale'])\n",
    "\n",
    "# Check\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.1 One hot encode Categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.get_dummies(data.drop(['item','description'], axis=1))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Preprocessing - Scaling, Test/Train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Define X,y\n",
    "X = data.drop(['target','first_sale'], axis=1)\n",
    "y = data.target\n",
    "\n",
    "# Preprocessing\n",
    "scaler = StandardScaler().fit(X)\n",
    "X_scaled = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training/testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = y.ravel()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, random_state = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check shapes\n",
    "print(X.shape)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5 - Modelling\n",
    "\n",
    "Now to build out 3 different models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 - Model 1: Random Forest\n",
    "\n",
    "Random forest is generally a good base model. It also has the added advantage of giving us feature importance, which can be translated to business impacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier()\n",
    "model_1 = rfc.fit(X_train, y_train)\n",
    "y_pred1 = model_1.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred1, digits=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.1 - Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Random forest param tuning\n",
    "param_grid = {\n",
    "    'n_estimators':np.arange(100,500,100),\n",
    "    'max_depth':[None, 3, 7, 11, 15],\n",
    "    'min_samples_leaf':np.arange(3,7,2)\n",
    "}\n",
    "\n",
    "rfc = RandomForestClassifier()\n",
    "gs1 = GridSearchCV(rfc, param_grid, verbose=5, cv=5, n_jobs=-1)\n",
    "\n",
    "gs1.results = gs1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best parameters: {}'.format(gs1.results.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit best model\n",
    "rfc = RandomForestClassifier(max_depth=None,\n",
    "                            min_samples_leaf=3,\n",
    "                            n_estimators=400)\n",
    "model1 = rfc.fit(X_train, y_train)\n",
    "y_pred1 = model1.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred1, digits=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.2 - RFC Feature Importances\n",
    "\n",
    "For this model, want to see the top features of importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print feature importances\n",
    "for name, importance in zip(data.drop(['target','first_sale'], axis=1), \n",
    "                            model_1.feature_importances_):\n",
    "    print(name, ': ', importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_importances = pd.Series(model_1.feature_importances_, \n",
    "                             index=data.drop(['target','first_sale'], axis=1).columns)\n",
    "feat_importances.sort_values(inplace=True, ascending=False)\n",
    "feat_importances[:10].plot(kind='barh')\n",
    "\n",
    "plt.title(\"Top 10 Feature Importances\")\n",
    "plt.savefig(\"figures/feature_importances.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 - Model 2: Support Vector Machine (SVM)\n",
    "\n",
    "SVM models are great in high dimensional space. Our data is relatively sparse due to the number of categorical features, so will be interesting to see how well this algorithm fits a model to our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svc = SVC()\n",
    "model_2 = svc.fit(X_train, y_train)\n",
    "y_pred2 = model_2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred2, digits=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.1 - Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVC parameter tuning\n",
    "param_grid = {\n",
    "    'kernel':['linear','rbf','poly'],\n",
    "    'gamma':[0.1,1,10,100],\n",
    "    'C':[0.1,1.0,10,100,1000]\n",
    "}\n",
    "\n",
    "gs2 = GridSearchCV(svc, param_grid, verbose=5, cv=5, n_jobs=-1)\n",
    "\n",
    "gs2.results = gs2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 - Model 3: Logistic Regression\n",
    "\n",
    "Logistic regression is a regression algorithm used for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "model_3 = lr.fit(X_train, y_train)\n",
    "y_pred3 = model_3.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred3, digits=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.1 - Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log Reg parameter tuning\n",
    "param_grid = {\n",
    "    'penalty':['l1','l2'],\n",
    "    'C':[0.1,1.0,10,100,1000]\n",
    "}\n",
    "\n",
    "gs3 = GridSearchCV(lr, param_grid, verbose=5, cv=5, n_jobs=-1)\n",
    "\n",
    "gs3.results = gs3.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
